---
format:
  revealjs:
    auto-stretch: false
    margin: 0
    slide-number: true
    scrollable: true
    preview-links: auto
    page-layout: custom
    logo: imagenes/logo_portada2.png
    css: ine_quarto_styles.css
    chalkboard: 
      boardmarker-width: 20
      buttons: false
    # footer: <https://quarto.org>
engine: knitr
---

#


[]{.linea-superior} 
[]{.linea-inferior} 

<!---
 <img src="imagenes/logo_portada2.png" style="width: 20%"/>  
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  


[**Modelo de detección de objetos YOLO**]{.big-par .center-justified}

[**Proyecto Ciencia de Datos**]{.medium-par.center-justified}

[**Unidad de Gobierno de Datos**]{.small-par.center-justified}

[**Diciembre 2024**]{.big-par .center-justified}



<!-- descripción de las cosas que se hacen con yolo -->
<!-- mostrar la variedad de modelos yolo que existen -->
<!-- arquitectura:  -->
<!-- - ¿Como se compone? ¿Que ocurre en cada una de esos ejes: Neck, BackBones, Head? -->
<!-- - comentar algunas partes de interés en cada una de esos ejes como el feature map, data augmentation etc -> que descomposiciones se debe realizar para que logremos capturar la imagen (procesamiento del input, muy a grandes rasgos (Neck)) -->

<!-- Performance (medidas) -->

<!-- mostrar el ejemplito de comparación entre versiones yolo -->

<!-- modelo YOLO-World ->  -->


## Contenidos  

::: {.medium-par}

- Problema de detección de objetos 🤔
  
  - Contexto 
  - Historia: Ventanas deslizantes y R-CNN

- ¿Qué es YOLO? 🤖
  
  - Arquitectura
  
  - Versiones
  
- Modelo YOLO-World 🌎
  
- Conclusiones 🚀

- Referencias 

:::

## Problema de detección de objetos (1/8)


[Miremos nuestro alrededor ¿cuántos objetos pueden identificar?]{.center .big-par}


[<img src="imagenes/plots/lupa.png" width="20%"/>]{.center} 

. . .

<!-- Los humanos, podemos reconocer objetos en imágenes en milisegundos. Miramos toda la imagen, entendemos el contexto, y sabemos que un objeto es x porque z y no flotando en el aire. Pero las computadoras no tienen esa intuición. Necesitan que les demos reglas claras para identificar qué están viendo y dónde lo están viendo -->

[¿Cómo le enseñamos a una máquina no solo a reconocer objetos, sino a decirnos exactamente dónde están?]{.center .big-par}

## Problema de detección de objetos (2/8)

::: {.incremental .medium-par}

- Uno de los primeros enfoques que abordaron esta problemática son las *ventanas deslizantes*

- Consiste en mover sistemáticamente una ventana de tamaño fijo a lo largo de una imagen (d x d pixeles) y clasificar el objeto dentro de la ventana utilizando una red de clasificación.

    [<img src="imagenes/plots/ventana_deslizante.PNG" width="60%"/>]{.center} 

:::


## Problema de detección de objetos (3/8)

Primera iteración: 

[<img src="imagenes/plots/ventana_deslizante1.PNG" width="70%"/>]{.center} 

[<img src="imagenes/plots/ventana_deslizante2.PNG" width="70%"/>]{.center} 


## Problema de detección de objetos (4/8)

Segunda iteración: 

[<img src="imagenes/plots/ventana_deslizante3.PNG" width="70%"/>]{.center} 

[<img src="imagenes/plots/ventana_deslizante4.PNG" width="70%"/>]{.center} 


## Problema de detección de objetos (5/8)

[<img src="imagenes/plots/ventana_deslizante5.PNG" width="70%"/>]{.center} 

:::{.incremental .medium-par}

- ¿Qué pasa si un mismo objeto es detectado más de una vez?

- ¿Cómo definimos el tamaño de nuestra ventana?

- ¿Cuanto muevo mi ventana?

- ¿Efectivamente mi objeto está en esas coordenadas?

:::

<!-- No obtenemos directamente las coordenadas exctas de nuestra imagen -->
<!-- Proceso computacionalmente muy costoso -->


## Problema de detección de objetos (6/8)

:::{.incremental .medium-par}

- A partir de esto, nace la idea de quedarnos con zonas que posiblemente tengan objetos
    
    [<img src="imagenes/plots/perro_gato_identif.PNG" width="50%"/>]{.center} 
    
::: 

## Problema de detección de objetos (7/8)

:::{.incremental .medium-par}

- Redes neuronales convolucionales basadas en regiones *R-CNN* (2014)

    [<img src="imagenes/plots/rcnn.PNG" width=90%"/>]{.center} 

- Se determinan regiones propuestas en donde posiblemente existen objetos

- Lo que nos ayuda a reducir la cantidad de ventanas a clasificar

- Estas propuestas de regiones se pueden seleccionar mediante la extracción de bordes, texturas, puntos característicos etc.

::: 

## Problema de detección de objetos (8/8)

:::{.incremental .medium-par}

- Si bien reducimos la cantidad de ventanas, sigue siendo lento para la detección en tiempo real

- Con las R-CNN continuamos procesando cada región por separado, lo que implica perdida del contexto de la imagen

::: 

. . .

[¿Por qué no miramos toda la imagen de una vez?🤔]{.center} 

## ¿Qué es YOLO? (1/3)

:::{.incremental .medium-par}

- Se propone You Only Look Once; solo se ve una vez. 

- Se observa toda la imagen y detectamos los objetos simultáneamente.

    [<img src="imagenes/plots/yolo1.PNG" width=90%"/>]{.center} 
    
- ¿Trabajar directamente con toda la imagen no es computacionalmente más costo?
  
  - Se realiza un downsampling, el cual reduce progresivamente el tamaño de las características de la imagen sin perder información relevante, haciendo que el modelo sea más eficiente.
  
  - A medida que la resolución disminuye, la red profundiza su entendimiento, analizando desde detalles finos (bordes) hasta patrones globales (formas completas).
  
:::




<!-- - Trabajar directamente con imágenes grandes sería computacionalmente costoso, tanto en términos de memoria como de tiempo. -->


<!-- Stride en las convoluciones: -->
<!-- Cada capa convolucional tiene un "stride" que controla cuánto se reduce la imagen en cada paso. -->

<!-- Ejemplo: Un stride de 2 reduce las dimensiones de la imagen a la mitad. -->
<!-- Pooling: -->
<!-- Operaciones como max pooling resumen regiones de la imagen seleccionando el valor más relevante (máximo). -->

<!-- Análisis jerárquico: -->
<!-- A medida que la resolución disminuye, la red profundiza su entendimiento, analizando desde detalles finos (bordes) hasta patrones globales (formas completas). -->



## ¿Qué es YOLO? (2/3)

:::{.incremental .medium-par}

- Para ello, se divide la imagen en cuadriculas (SxS), en donde cada celda es responsable de detectar objetos que caigan dentro su región

    [<img src="imagenes/plots/yolo2.PNG" width=70%"/>]{.center} 


- Se realiza la predicción en cada celda, ¿existe un objeto aquí? ¿cuál? -> retorna las coordenadas

- Posteriormente se realiza una limpieza de celdas *Non-Max Suppression*
  
  <!-- 1. Eliminar los bounding box que tengan una probabilidad menor a un umbral -->

  <!-- 2. De los bounding boxing restantes, se toma el valor mayor predicción -->

  <!-- 3. Se calcula el IoU entre el bounding box mayor con los demás -->

  <!-- 4. Si el IoU es mayor a cierto umbral, entonces vamos a descartarlos pues ya están siendo explicados por la mayor predicción -->

  <!-- 5. Iterar los pasos 2 al 4 -->
  
  <!-- IoU: Intersección sobre la unión Inter/Unión -> para ver que tan buena es la predicción -->
::: 

## ¿Qué es YOLO? (3/3)

:::{.incremental .medium-par}

- Para entrenar al modelo, muy a grandes rasgos:

    [<img src="imagenes/plots/yolo3.PNG" width=70%"/>]{.center} 

:::





<!-- - Pasamos toda la información de la imagen por la red y obtenemos las coordenadas con sus clases correspondientes para cada objeto -->




<!-- YOLO: -->
<!--   - Dividir imagen, se divide en cuadriculas (SxS), en donde cada celda es responsable de detectar objetos que caigan dentro de la region -->
<!--   - Predicción en cada celda, ¿Existe un objeto aqui? ¿cuál? -> retorna las coordenadas -->
<!--   - Limpieza de los Bounding Boxes, las celdas cercanas pueden predecir el mismo objeto con pequeñas diferencias -> se eliminan las predicciones supuestas descartando las menos confiables con Non-Maximum Suppression -->





<!-- ## Comparación YOLO vs Métodos Anteriores -->
<!-- Velocidad: -->

<!-- R-CNN procesa regiones por separado; YOLO procesa todo de una vez. -->
<!-- Resultado: YOLO es significativamente más rápido. -->
<!-- Precisión: -->

<!-- Los modelos anteriores no usan el contexto global de la imagen; YOLO sí. -->
<!-- Resultado: Mejor detección en escenarios complejos (como objetos solapados). -->
<!-- Simplicidad: -->

<!-- YOLO convierte la detección de objetos en un solo problema de regresión. -->
<!-- Resultado: Una arquitectura más simple, fácil de entrenar y más eficiente. -->
  
  
<!-- https://blog.damavis.com/reconocimiento-de-objetos-con-deep-learning/ -->
  
<!-- Detección de objetos con YOLO: implementaciones y como usarlas -->

<!-- https://medium.com/@Data_Aficionado_1083/object-detection-sliding-window-r-cnn-fast-r-cnn-faster-r-cnn-f47c7dbe003d -->


## ¿Qué es YOLO? | Arquitectura (1/2)

:::{.incremental .medium-par}

- El modelo consta de una arquitectura de 3 partes principales: Backbone, Neck, Head

    [<img src="imagenes/plots/yolo_struture2.PNG" width=90%"/>]{.center} 

- Backbone: Extrae características del input (patrones, texturas y bordes) en diferentes niveles de abstracción a un conjunto llamado mapas de características (feature maps)

- Neck: Organiza y refina estas características, mejorando la información espacial

- Head: Produce las predicciones finales y entrega el bounding box

<!-- - Esta estructura modular es lo que permite la velocidad y precisión de YOLO. -->
:::


<!-- El Backbone "traduce" la imagen original a un conjunto de mapas de características (feature maps) que representan diferentes niveles de abstracción. -->




## ¿Qué es YOLO? | Arquitectura (2/2)

YOLOv5:

[<img src="imagenes/plots/estructura_yolov5.avif" width=70%"/>]{.center} 


## ¿Qué es YOLO? | Versiones (1/2)

[<img src="imagenes/plots/yolo_timeline.png" width="90%"/>]{.center}

## ¿Qué es YOLO? | Versiones (2/2)

[<img src="imagenes/plots/comparaacion_yolovs.png" width="90%"/>]{.center}

. . .

Para la gran mayoría de modelos YOLO existentes no hay papers formales, pero si podemos encontrar documentación de su uso
  
  [<img src="imagenes/plots/papers.png" width="70%"/>]{.center}


## Modelo YOLO-World (1/2)

:::{.incremental .medium-par}

- Sabemos que para detectar los objetos necesitamos una clase fija, pero ¿podremos detectar objetos a partir de textos descriptivos? 

  
  > YOLO-World presenta el paradigma prompt-then-detect, *escribo luego detecto*, para una inferencia eficiente basada en el vocabulario del usuario el cual re-parametriza embeddings como parámetros dentro del modelo y logra una velocidad de inferencia *superior*.

    [<img src="imagenes/plots/yolo_world.png" width="90%"/>]{.center}
    
::: 

## Modelo YOLO-World (2/2)

Ejemplos:

. . .

[<img src="imagenes/plots/yolo_world_ex.png" width="90%"/>]{.center}

[<img src="imagenes/plots/yolo_world_ex2.png" width="90%"/>]{.center}


## Conclusiones 🚀

:::{.incremental .medium-par}

- Como hemos visto, YOLO es un tanque

:::


## Referencias
 

:::{.medium-par}

- [Ultralytics](https://docs.ultralytics.com/models/)

- [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640)

- [YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/pdf/2004.10934v1)

- [YOLOV5, YOLOV8 AND YOLOV10: THE GO-TO DETECTORS FOR REAL-TIME VISION](https://arxiv.org/pdf/2407.02988)

- [YOLO-World: Real-Time Open-Vocabulary Object Detection](https://arxiv.org/pdf/2401.17270)

- [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/pdf/1311.2524)

<!-- https://www.aprendemachinelearning.com/modelos-de-deteccion-de-objetos/ -->

:::

#


[]{.linea-superior} 
[]{.linea-inferior} 


<img src="imagenes/logo_portada2.png" width="20%"/>  


[**Modelo de detección de objetos YOLO**]{.big-par .center-justified}

[**Proyecto Ciencia de Datos**]{.medium-par.center-justified}

[**Unidad de Gobierno de Datos**]{.small-par.center-justified}

[**Diciembre 2024**]{.big-par .center-justified}


